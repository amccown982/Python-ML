{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vectors, Matrices, and Arrays in NumPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NumPy is a foundational tool of the Python machine learning stack. NumPy allows for efficient operations on the data structures often used in machine learning: vectors, matrices, and tensors. While NumPy isn’t the focus of this book, it will show up frequently in the following chapters. This chapter covers the most common NumPy operations we’re likely to run into while working on machine learning workflows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load library\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3]\n"
     ]
    }
   ],
   "source": [
    "# Create a vector as a row\n",
    "vector_row = np.array([1, 2, 3])\n",
    "\n",
    "print(vector_row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1]\n",
      " [2]\n",
      " [3]]\n"
     ]
    }
   ],
   "source": [
    "# Create a vector as a column\n",
    "vector_column = np.array([[1],\n",
    "                          [2],\n",
    "                          [3]])\n",
    "\n",
    "print(vector_column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NumPy’s main data structure is the multidimensional array. A vector is just an array with a single dimension. To create a vector, we simply create a one-dimensional array. Just like vectors, these arrays can be represented horizontally (i.e., rows) or vertically (i.e., columns)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load library\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [1 2]\n",
      " [1 2]]\n"
     ]
    }
   ],
   "source": [
    "# Create a matrix\n",
    "matrix = np.array([[1, 2],\n",
    "                   [1, 2],\n",
    "                   [1, 2]])\n",
    "\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create a matrix we can use a NumPy two-dimensional array. In our solution, the matrix contains three rows and two columns (a column of 1s and a column of 2s).\n",
    "\n",
    "NumPy actually has a dedicated matrix data structure:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 2]\n",
      " [1 2]\n",
      " [1 2]]\n"
     ]
    }
   ],
   "source": [
    "matrix_object = np.mat([[1, 2],\n",
    "                        [1, 2],\n",
    "                        [1, 2]])\n",
    "\n",
    "print(matrix_object)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, the matrix data structure is not recommended for two reasons. First, arrays are the de facto standard data structure of NumPy. Second, the vast majority of NumPy operations return arrays, not matrix objects."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Sparse Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install scipy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load library\n",
    "import numpy as np\n",
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0]\n",
      " [0 1]\n",
      " [3 0]]\n"
     ]
    }
   ],
   "source": [
    "# Create a matrix\n",
    "matrix = np.array([[0, 0],\n",
    "                   [0, 1],\n",
    "                   [3, 0]])\n",
    "\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A frequent situation in machine learning is having a huge amount of data; however, most of the elements in the data are zeros. For example, imagine a matrix where the columns are every movie on Netflix, the rows are every Netflix user, and the values are how many times a user has watched that particular movie. This matrix would have tens of thousands of columns and millions of rows! However, since most users do not watch most movies, the vast majority of elements would be zero.\n",
    "\n",
    "A sparse matrix is a matrix in which most elements are 0. Sparse matrices store only nonzero elements and assume all other values will be zero, leading to significant computational savings. In our solution, we created a NumPy array with two nonzero values, then converted it into a sparse matrix. If we view the sparse matrix we can see that only the nonzero values are stored:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (1, 1)\t1\n",
      "  (2, 0)\t3\n"
     ]
    }
   ],
   "source": [
    "# Create compressed sparse row (CSR) matrix\n",
    "matrix_sparse = sparse.csr_matrix(matrix)\n",
    "\n",
    "print(matrix_sparse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are a number of types of sparse matrices. However, in compressed sparse row (CSR) matrices, (1, 1) and (2, 0) represent the (zero-indexed) indices of the nonzero values 1 and 3, respectively. For example, the element 1 is in the second row and second column. We can see the advantage of sparse matrices if we create a much larger matrix with many more zero elements and then compare this larger matrix with our original sparse matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 0 0 0 0 0]\n",
      " [0 1 0 0 0 0 0 0 0 0]\n",
      " [3 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# Create larger matrix\n",
    "matrix_large = np.array([[0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                         [0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                         [3, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
    "\n",
    "print(matrix_large)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (1, 1)\t1\n",
      "  (2, 0)\t3\n",
      "\n",
      "\n",
      "  (1, 1)\t1\n",
      "  (2, 0)\t3\n"
     ]
    }
   ],
   "source": [
    "# Create compressed sparse row (CSR) matrix\n",
    "matrix_large_sparse = sparse.csr_matrix(matrix_large)\n",
    "# View original sparse matrix\n",
    "print(matrix_sparse)\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "# View larger sparse matrix\n",
    "print(matrix_large_sparse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, despite the fact that we added many more zero elements in the larger matrix, its sparse representation is exactly the same as our original sparse matrix. That is, the addition of zero elements did not change the size of the sparse matrix.\n",
    "\n",
    "As mentioned, there are many different types of sparse matrices, such as compressed sparse column, list of lists, and dictionary of keys. While an explanation of the different types and their implications is outside the scope of this book, it is worth noting that while there is no “best” sparse matrix type, there are meaningful differences among them, and we should be conscious about why we are choosing one type over another."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preallocating NumPy Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load library\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Generate a vector of shape (1,5) containing all zeros\n",
    "vector = np.zeros(shape=5)\n",
    "\n",
    "print(vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1]\n",
      " [1 1 1]\n",
      " [1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "# Generate a matrix of shape (3,3) containing all ones\n",
    "matrix = np.full(shape=(3,3), fill_value=1)\n",
    "\n",
    "print(matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating arrays prefilled with data is useful for a number of purposes, such as making code more performant or using synthetic data to test algorithms. In many programming languages, preallocating an array of default values (such as 0s) is considered common practice."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting Elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load library\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "\n",
      "\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "# Create row vector\n",
    "vector = np.array([1, 2, 3, 4, 5, 6])\n",
    "\n",
    "# Create matrix\n",
    "matrix = np.array([[1, 2, 3],\n",
    "                   [4, 5, 6],\n",
    "                   [7, 8, 9]])\n",
    "\n",
    "# Select third element of vector\n",
    "print(vector[2])\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "# Select second row, second column\n",
    "print(matrix[1,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like most things in Python, NumPy arrays are zero-indexed, meaning that the index of the first element is 0, not 1. With that caveat, NumPy offers a wide variety of methods for selecting (i.e., indexing and slicing) elements or groups of elements in arrays:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 4, 5, 6])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select all elements of a vector\n",
    "vector[:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select everything up to and including the third element\n",
    "vector[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4, 5, 6])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select everything after the third element\n",
    "vector[3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select the last element\n",
    "vector[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6, 5, 4, 3, 2, 1])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reverse the vector\n",
    "vector[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3],\n",
       "       [4, 5, 6]])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select the first two rows and all columns of a matrix\n",
    "matrix[:2,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2],\n",
       "       [ 5],\n",
       "       [ 8],\n",
       "       [11]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select all rows and the second column\n",
    "matrix[:,1:2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Describing a Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load library\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create matrix\n",
    "matrix = np.array([[1, 2, 3, 4],\n",
    "                   [5, 6, 7, 8],\n",
    "                   [9, 10, 11, 12]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 4)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View number of rows and columns\n",
    "matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View number of elements (rows * columns)\n",
    "matrix.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View number of dimensions\n",
    "matrix.ndim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This might seem basic (and it is); however, time and again it will be valuable to check the shape and size of an array both for further calculations and simply as a gut check after an operation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying Functions over Each Element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load library\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create matrix\n",
    "matrix = np.array([[1, 2, 3],\n",
    "                   [4, 5, 6],\n",
    "                   [7, 8, 9]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function that adds 100 to something\n",
    "add_100 = lambda i: i + 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create vectorized function\n",
    "vectorized_add_100 = np.vectorize(add_100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[101, 102, 103],\n",
       "       [104, 105, 106],\n",
       "       [107, 108, 109]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Apply function to all elements in matrix\n",
    "vectorized_add_100(matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The NumPy vectorize method converts a function into a function that can apply to all elements in an array or slice of an array. It’s worth noting that vectorize is essentially a for loop over the elements and does not increase performance. Furthermore, NumPy arrays allow us to perform operations between arrays even if their dimensions are not the same (a process called broadcasting). For example, we can create a much simpler version of our solution using broadcasting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[101, 102, 103],\n",
       "       [104, 105, 106],\n",
       "       [107, 108, 109],\n",
       "       [110, 111, 112]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add 100 to all elements\n",
    "matrix + 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Broadcasting does not work for all shapes and situations, but it is a common way of applying simple operations over all elements of a NumPy array."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the Maximum and Minimum Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load library\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create matrix\n",
    "matrix = np.array([[1, 2, 3],\n",
    "                   [4, 5, 6],\n",
    "                   [7, 8, 9]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Return maximum element\n",
    "np.max(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Return minimum element\n",
    "np.min(matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often we want to know the maximum and minimum value in an array or subset of an array. This can be accomplished with the max and min methods. Using the axis parameter, we can also apply the operation along a certain axis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([7, 8, 9])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find maximum element in each column\n",
    "np.max(matrix, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 6, 9])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find maximum element in each row\n",
    "np.max(matrix, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating the Average, Variance, and Standard Deviation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load library\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create matrix\n",
    "matrix = np.array([[1, 2, 3],\n",
    "                   [4, 5, 6],\n",
    "                   [7, 8, 9]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5.0"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Return mean\n",
    "np.mean(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6.666666666666667"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Return variance\n",
    "np.var(matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.581988897471611"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Return standard deviation\n",
    "np.std(matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just like with max and min, we can easily get descriptive statistics about the whole matrix or do calculations along a single axis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([4., 5., 6.])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the mean value in each column\n",
    "np.mean(matrix, axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reshaping Arrays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load library\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 4x3 matrix\n",
    "matrix = np.array([[1, 2, 3],\n",
    "                   [4, 5, 6],\n",
    "                   [7, 8, 9],\n",
    "                   [10, 11, 12]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  2,  3,  4,  5,  6],\n",
       "       [ 7,  8,  9, 10, 11, 12]])"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Reshape matrix into 2x6 matrix\n",
    "matrix.reshape(2, 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reshape allows us to restructure an array so that we maintain the same data but organize it as a different number of rows and columns. The only requirement is that the shape of the original and new matrix contain the same number of elements (i.e., are the same size). We can see the size of a matrix using size:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One useful argument in reshape is -1, which effectively means “as many as needed,” so reshape(1, -1) means one row and as many columns as needed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12]])"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix.reshape(1, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, if we provide one integer, reshape will return a one-dimensional array of that length:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix.reshape(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transposing a Vector or Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load library\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create matrix\n",
    "matrix = np.array([[1, 2, 3],\n",
    "                   [4, 5, 6],\n",
    "                   [7, 8, 9]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 4, 7],\n",
       "       [2, 5, 8],\n",
       "       [3, 6, 9]])"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transpose matrix\n",
    "matrix.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transposing is a common operation in linear algebra where the column and row indices of each element are swapped. A nuanced point typically overlooked outside of a linear algebra class is that, technically, a vector can’t be transposed because it’s just a collection of values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 4, 5, 6])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transpose vector\n",
    "np.array([1, 2, 3, 4, 5, 6]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, it is common to refer to transposing a vector as converting a row vector to a column vector (notice the second pair of brackets) or vice versa:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1],\n",
       "       [2],\n",
       "       [3],\n",
       "       [4],\n",
       "       [5],\n",
       "       [6]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Transpose row vector\n",
    "np.array([[1, 2, 3, 4, 5, 6]]).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Flattening a Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load library\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create matrix\n",
    "matrix = np.array([[1, 2, 3],\n",
    "                   [4, 5, 6],\n",
    "                   [7, 8, 9]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Flatten matrix\n",
    "matrix.flatten()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "flatten is a simple method to transform a matrix into a one-dimensional array. Alternatively, we can use reshape to create a row vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3, 4, 5, 6, 7, 8, 9]])"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matrix.reshape(1, -1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another common way to flatten arrays is the ravel method. Unlike flatten, which returns a copy of the original array, ravel operates on the original object itself and is therefore slightly faster. It also lets us flatten lists of arrays, which we can’t do with the flatten method. This operation is useful for flattening very large arrays and speeding up code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create one matrix\n",
    "matrix_a = np.array([[1, 2],\n",
    "                     [3, 4]])\n",
    "\n",
    "# Create a second matrix\n",
    "matrix_b = np.array([[5, 6],\n",
    "                     [7, 8]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of matrices\n",
    "matrix_list = [matrix_a, matrix_b]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3, 4, 5, 6, 7, 8])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Flatten the entire list of matrices\n",
    "np.ravel(matrix_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding the Rank of a Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load library\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create matrix\n",
    "matrix = np.array([[1, 1, 1],\n",
    "                   [1, 1, 10],\n",
    "                   [1, 1, 15]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Return matrix rank\n",
    "np.linalg.matrix_rank(matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rank of a matrix is the dimensions of the vector space spanned by its columns or rows. Finding the rank of a matrix is easy in NumPy thanks to matrix_rank."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the Diagonal of a Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load library\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create matrix\n",
    "matrix = np.array([[1, 2, 3],\n",
    "                   [2, 4, 6],\n",
    "                   [3, 8, 9]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 4, 9])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Return diagonal elements\n",
    "matrix.diagonal()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NumPy makes getting the diagonal elements of a matrix easy with diagonal. It is also possible to get a diagonal off the main diagonal by using the offset parameter:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 6])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Return diagonal one above the main diagonal\n",
    "matrix.diagonal(offset=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 8])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Return diagonal one below the main diagonal\n",
    "matrix.diagonal(offset=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating the Trace of a Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load library\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create matrix\n",
    "matrix = np.array([[1, 2, 3],\n",
    "                   [2, 4, 6],\n",
    "                   [3, 8, 9]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Return trace\n",
    "matrix.trace()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The trace of a matrix is the sum of the diagonal elements and is often used under the hood in machine learning methods. Given a NumPy multidimensional array, we can calculate the trace using trace. Alternatively, we can return the diagonal of a matrix and calculate its sum:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "14"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Return diagonal and sum elements\n",
    "sum(matrix.diagonal())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calcualting the Dot Products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load library\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create two vectors\n",
    "vector_a = np.array([1,2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_b = np.array([4,5,6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate dot product\n",
    "np.dot(vector_a, vector_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dot product of two vectors,  and , is defined as:\n",
    "n\n",
    "E aibi\n",
    "i=1 \n",
    "\n",
    "where \n",
    " is the th element of vector , and \n",
    " is the th element of vector . We can use NumPy’s dot function to calculate the dot product. Alternatively, in Python 3.5+ we can use the new @ operator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate dot product\n",
    "vector_a @ vector_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adding and Subtracting Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load library\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Create matrix\n",
    "matrix_a = np.array([[1, 1, 1],\n",
    "                     [1, 1, 1],\n",
    "                     [1, 1, 2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create matrix\n",
    "matrix_b = np.array([[1, 3, 1],\n",
    "                     [1, 3, 1],\n",
    "                     [1, 3, 8]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2,  4,  2],\n",
       "       [ 2,  4,  2],\n",
       "       [ 2,  4, 10]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add two matrices\n",
    "np.add(matrix_a, matrix_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0, -2,  0],\n",
       "       [ 0, -2,  0],\n",
       "       [ 0, -2, -6]])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Subtract two matrices\n",
    "np.subtract(matrix_a, matrix_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we can simply use the + and – operators:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 2,  4,  2],\n",
       "       [ 2,  4,  2],\n",
       "       [ 2,  4, 10]])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add two matrices\n",
    "matrix_a + matrix_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multiplying Matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load library\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create matrix\n",
    "matrix_a = np.array([[1, 1],\n",
    "                     [1, 2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create matrix\n",
    "matrix_b = np.array([[1, 3],\n",
    "                     [1, 2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 5],\n",
       "       [3, 7]])"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Multiply two matrices\n",
    "np.dot(matrix_a, matrix_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, in Python 3.5+ we can use the @ operator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 5],\n",
       "       [3, 7]])"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Multiply two matrices\n",
    "matrix_a @ matrix_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to do element-wise multiplication, we can use the * operator:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 3],\n",
       "       [1, 4]])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Multiply two matrices element-wise\n",
    "matrix_a * matrix_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inverting a Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load library\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create matrix\n",
    "matrix = np.array([[1, 4],\n",
    "                   [2, 5]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.66666667,  1.33333333],\n",
       "       [ 0.66666667, -0.33333333]])"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate inverse of matrix\n",
    "np.linalg.inv(matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The inverse of a square matrix, A, is a second matrix, A–1, such that:\n",
    "\n",
    "where I is the identity matrix. In NumPy we can use linalg.inv to calculate A–1 if it exists. To see this in action, we can multiply a matrix by its inverse, and the result is the identity matrix:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.00000000e+00, 0.00000000e+00],\n",
       "       [1.11022302e-16, 1.00000000e+00]])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Multiply matrix and its inverse\n",
    "matrix @ np.linalg.inv(matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating Random Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load library\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5488135 , 0.71518937, 0.60276338])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate three random floats between 0.0 and 1.0\n",
    "np.random.random(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NumPy offers a wide variety of means to generate random numbers—many more than can be covered here. In our solution we generated floats; however, it is also common to generate integers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([3, 7, 9])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Generate three random integers between 0 and 10\n",
    "np.random.randint(0, 11, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we can generate numbers by drawing them from a distribution (note this is not technically random):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.42232584,  1.52006949, -0.29139398])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Draw three numbers from a normal distribution with mean 0.0\n",
    "# and standard deviation of 1.0\n",
    "np.random.normal(0.0, 1.0, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.98118713, -0.08939902,  1.46416405])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Draw three numbers from a logistic distribution with mean 0.0 and scale of 1.0\n",
    "np.random.logistic(0.0, 1.0, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.47997717, 1.3927848 , 1.83607876])"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Draw three numbers greater than or equal to 1.0 and less than 2.0\n",
    "np.random.uniform(1.0, 2.0, 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, sometimes it can be useful to return the same random numbers multiple times to get predictable, repeatable results. We can do this by setting the “seed” (an integer) of the pseudorandom generator. Random processes with the same seed will always produce the same output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first step in any machine learning endeavor is to get the raw data into our system. The raw data might be a logfile, dataset file, database, or cloud blob store such as Amazon S3. Furthermore, often we will want to retrieve data from multiple sources.\n",
    "\n",
    "The recipes in this chapter look at methods of loading data from a variety of sources, including CSV files and SQL databases. We also cover methods of generating simulated data with desirable properties for experimentation. Finally, while there are many ways to load data in the Python ecosystem, we will focus on using the pandas library’s extensive set of methods for loading external data, and using scikit-learn—​an open source machine learning library in Python—​for generating simulated data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading a Sample Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.4.1.post1-cp311-cp311-win_amd64.whl (10.6 MB)\n",
      "                                              0.0/10.6 MB ? eta -:--:--\n",
      "                                              0.2/10.6 MB 7.3 MB/s eta 0:00:02\n",
      "     ----                                     1.3/10.6 MB 16.5 MB/s eta 0:00:01\n",
      "     ---------------------                    5.7/10.6 MB 45.4 MB/s eta 0:00:01\n",
      "     ------------------------------------     9.7/10.6 MB 56.6 MB/s eta 0:00:01\n",
      "     --------------------------------------  10.6/10.6 MB 73.1 MB/s eta 0:00:01\n",
      "     --------------------------------------- 10.6/10.6 MB 54.7 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy<2.0,>=1.19.5 in c:\\users\\mccow\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn) (1.24.3)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\mccow\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from scikit-learn) (1.13.0)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Downloading joblib-1.3.2-py3-none-any.whl (302 kB)\n",
      "                                              0.0/302.2 kB ? eta -:--:--\n",
      "     ---------------------------------------- 302.2/302.2 kB ? eta 0:00:00\n",
      "Collecting threadpoolctl>=2.0.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.4.0-py3-none-any.whl (17 kB)\n",
      "Installing collected packages: threadpoolctl, joblib, scikit-learn\n",
      "Successfully installed joblib-1.3.2 scikit-learn-1.4.1.post1 threadpoolctl-3.4.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# !pip install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load scikit-learn's datasets\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load digits dataset\n",
    "digits = datasets.load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create features matrix\n",
    "features = digits.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create target vector\n",
    "target = digits.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.,  0.,  5., 13.,  9.,  1.,  0.,  0.,  0.,  0., 13., 15., 10.,\n",
       "       15.,  5.,  0.,  0.,  3., 15.,  2.,  0., 11.,  8.,  0.,  0.,  4.,\n",
       "       12.,  0.,  0.,  8.,  8.,  0.,  0.,  5.,  8.,  0.,  0.,  9.,  8.,\n",
       "        0.,  0.,  4., 11.,  0.,  1., 12.,  7.,  0.,  0.,  2., 14.,  5.,\n",
       "       10., 12.,  0.,  0.,  0.,  0.,  6., 13., 10.,  0.,  0.,  0.])"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View first observation\n",
    "features[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often we do not want to go through the work of loading, transforming, and cleaning a real-world dataset before we can explore some machine learning algorithm or method. Luckily, scikit-learn comes with some common datasets we can quickly load.\n",
    "\n",
    "These datasets are often called “toy” datasets because they are far smaller and cleaner than a dataset we would see in the real world. Some popular sample datasets in scikit-learn are:\n",
    "\n",
    "load_iris\n",
    "Contains 150 observations on the measurements of iris flowers. It is a good dataset for exploring classification algorithms.\n",
    "\n",
    "load_digits\n",
    "Contains 1,797 observations from images of handwritten digits. It is a good dataset for teaching image classification.\n",
    "\n",
    "To see more details on any of these datasets, you can print the DESCR attribute:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load scikit-learn's datasets\n",
    "from sklearn import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load digits dataset\n",
    "digits = datasets.load_digits()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".. _digits_dataset:\n",
      "\n",
      "Optical recognition of handwritten digits dataset\n",
      "--------------------------------------------------\n",
      "\n",
      "**Data Set Characteristics:**\n",
      "\n",
      ":Number of Instances: 1797\n",
      ":Number of Attributes: 64\n",
      ":Attribute Information: 8x8 image of integer pixels in the range 0..16.\n",
      ":Missing Attribute Values: None\n",
      ":Creator: E. Alpaydin (alpaydin '@' boun.edu.tr)\n",
      ":Date: July; 1998\n",
      "\n",
      "This is a copy of the test set of the UCI ML hand-written digits datasets\n",
      "https://archive.ics.uci.edu/ml/datasets/Optical+Recognition+of+Handwritten+Digits\n",
      "\n",
      "The data set contains images of hand-written digits: 10 classes where\n",
      "each class refers to a digit.\n",
      "\n",
      "Preprocessing programs made available by NIST were used to extract\n",
      "normalized bitmaps of handwritten digits from a preprinted form. From a\n",
      "total of 43 people, 30 contributed to the training set and different 13\n",
      "to the test set. 32x32 bitmaps are divided into nonoverlapping blocks of\n",
      "4x4 and the number of on pixels are counted in each block. This generates\n",
      "an input matrix of 8x8 where each element is an integer in the range\n",
      "0..16. This reduces dimensionality and gives invariance to small\n",
      "distortions.\n",
      "\n",
      "For info on NIST preprocessing routines, see M. D. Garris, J. L. Blue, G.\n",
      "T. Candela, D. L. Dimmick, J. Geist, P. J. Grother, S. A. Janet, and C.\n",
      "L. Wilson, NIST Form-Based Handprint Recognition System, NISTIR 5469,\n",
      "1994.\n",
      "\n",
      "|details-start|\n",
      "**References**\n",
      "|details-split|\n",
      "\n",
      "- C. Kaynak (1995) Methods of Combining Multiple Classifiers and Their\n",
      "  Applications to Handwritten Digit Recognition, MSc Thesis, Institute of\n",
      "  Graduate Studies in Science and Engineering, Bogazici University.\n",
      "- E. Alpaydin, C. Kaynak (1998) Cascading Classifiers, Kybernetika.\n",
      "- Ken Tang and Ponnuthurai N. Suganthan and Xi Yao and A. Kai Qin.\n",
      "  Linear dimensionalityreduction using relevance weighted LDA. School of\n",
      "  Electrical and Electronic Engineering Nanyang Technological University.\n",
      "  2005.\n",
      "- Claudio Gentile. A New Approximate Maximal Margin Classification\n",
      "  Algorithm. NIPS. 2000.\n",
      "\n",
      "|details-end|\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Print the attribute\n",
    "print(digits.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating a Simulated Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "scikit-learn offers many methods for creating simulated data. Of those, three methods are particularly useful: make_regression, make_classification, and make_blobs.\n",
    "\n",
    "When we want a dataset designed to be used with linear regression, make_regression is a good choice:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load library\n",
    "from sklearn.datasets import make_regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate features matrix, target vector, and the true coefficients\n",
    "features, target, coefficients = make_regression(n_samples = 100,\n",
    "                                                 n_features = 3,\n",
    "                                                 n_informative = 3,\n",
    "                                                 n_targets = 1,\n",
    "                                                 noise = 0.0,\n",
    "                                                 coef = True,\n",
    "                                                 random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Matrix\n",
      " [[ 1.29322588 -0.61736206 -0.11044703]\n",
      " [-2.793085    0.36633201  1.93752881]\n",
      " [ 0.80186103 -0.18656977  0.0465673 ]]\n",
      "Target Vector\n",
      " [-10.37865986  25.5124503   19.67705609]\n"
     ]
    }
   ],
   "source": [
    "# View feature matrix and target vector\n",
    "print('Feature Matrix\\n', features[:3])\n",
    "print('Target Vector\\n', target[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we are interested in creating a simulated dataset for classification, we can use make_classification:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load library\n",
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate features matrix and target vector\n",
    "features, target = make_classification(n_samples = 100,\n",
    "                                       n_features = 3,\n",
    "                                       n_informative = 3,\n",
    "                                       n_redundant = 0,\n",
    "                                       n_classes = 2,\n",
    "                                       weights = [.25, .75],\n",
    "                                       random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Matrix\n",
      " [[ 1.06354768 -1.42632219  1.02163151]\n",
      " [ 0.23156977  1.49535261  0.33251578]\n",
      " [ 0.15972951  0.83533515 -0.40869554]]\n",
      "Target Vector\n",
      " [1 0 0]\n"
     ]
    }
   ],
   "source": [
    "# View feature matrix and target vector\n",
    "print('Feature Matrix\\n', features[:3])\n",
    "print('Target Vector\\n', target[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, if we want a dataset designed to work well with clustering techniques, scikit-learn offers make_blobs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load library\n",
    "from sklearn.datasets import make_blobs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate features matrix and target vector\n",
    "features, target = make_blobs(n_samples = 100,\n",
    "                              n_features = 2,\n",
    "                              centers = 3,\n",
    "                              cluster_std = 0.5,\n",
    "                              shuffle = True,\n",
    "                              random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Matrix\n",
      " [[ -1.22685609   3.25572052]\n",
      " [ -9.57463218  -4.38310652]\n",
      " [-10.71976941  -4.20558148]]\n",
      "Target Vector\n",
      " [0 1 1]\n"
     ]
    }
   ],
   "source": [
    "# View feature matrix and target vector\n",
    "print('Feature Matrix\\n', features[:3])\n",
    "print('Target Vector\\n', target[:3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As might be apparent from the solutions, make_regression returns a feature matrix of float values and a target vector of float values, while make_classification and make_blobs return a feature matrix of float values and a target vector of integers representing membership in a class.\n",
    "\n",
    "scikit-learn’s simulated datasets offer extensive options to control the type of data generated. scikit-learn’s documentation contains a full description of all the parameters, but a few are worth noting.\n",
    "\n",
    "In make_regression and make_classification, n_informative determines the number of features that are used to generate the target vector. If n_informative is less than the total number of features (n_features), the resulting dataset will have redundant features that can be identified through feature selection techniques.\n",
    "\n",
    "In addition, make_classification contains a weights parameter that allows us to simulate datasets with imbalanced classes. For example, weights = [.25, .75] would return a dataset with 25% of observations belonging to one class and 75% of observations belonging to a second class.\n",
    "\n",
    "For make_blobs, the centers parameter determines the number of clusters generated. Using the matplotlib visualization library, we can visualize the clusters generated by make_blobs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in c:\\users\\mccow\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.8.4)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\mccow\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (1.2.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\mccow\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\mccow\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\mccow\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (1.4.5)\n",
      "Requirement already satisfied: numpy>=1.21 in c:\\users\\mccow\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (1.24.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\mccow\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib) (23.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\mccow\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (10.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\mccow\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from matplotlib) (3.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\mccow\\appdata\\roaming\\python\\python311\\site-packages (from matplotlib) (2.8.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\mccow\\appdata\\roaming\\python\\python311\\site-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# !pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load library\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# View scatterplot\n",
    "plt.scatter(features[:,0], features[:,1], c=target)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading a CSV File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the pandas library’s read_csv to load a local or hosted CSV file into a pandas DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load library\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create URL\n",
    "url = 'https://raw.githubusercontent.com/chrisalbon/sim_data/master/data.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "dataframe = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>integer</th>\n",
       "      <th>datetime</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>2015-01-01 00:00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>2015-01-01 00:00:01</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   integer             datetime  category\n",
       "0        5  2015-01-01 00:00:00         0\n",
       "1        5  2015-01-01 00:00:01         0"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View first two rows\n",
    "dataframe.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two things to note about loading CSV files. First, it is often useful to take a quick look at the contents of the file before loading. It can be very helpful to see how a dataset is structured beforehand and what parameters we need to set to load in the file. Second, read_csv has over 30 parameters and therefore the documentation can be daunting. Fortunately, those parameters are mostly there to allow it to handle a wide variety of CSV formats.\n",
    "\n",
    "CSV files get their names from the fact that the values are literally separated by commas (e.g., one row might be 2,\"2015-01-01 00:00:00\",0); however, it is common for CSV files to use other separators, such as tabs (which are referred to as TSV files). The pandas sep parameter allows us to define the delimiter used in the file. Although it is not always the case, a common formatting issue with CSV files is that the first line of the file is used to define column headers (e.g., integer, datetime, category in our solution). The header parameter allows us to specify if or where a header row exists. If a header row does not exist, we set header=None.\n",
    "\n",
    "The read_csv function returns a pandas DataFrame: a common and useful object for working with tabular data that we’ll cover in more depth throughout this book."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading an Excel File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load library\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create URL\n",
    "url = 'https://raw.githubusercontent.com/chrisalbon/sim_data/master/data.xlsx'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Missing optional dependency 'openpyxl'.  Use pip or conda to install openpyxl.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\mccow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\compat\\_optional.py:142\u001b[0m, in \u001b[0;36mimport_optional_dependency\u001b[1;34m(name, extra, errors, min_version)\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 142\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\mccow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\importlib\\__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1206\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1178\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1142\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'openpyxl'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[142], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Load data\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m dataframe \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_excel\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msheet_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mccow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:478\u001b[0m, in \u001b[0;36mread_excel\u001b[1;34m(io, sheet_name, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skiprows, nrows, na_values, keep_default_na, na_filter, verbose, parse_dates, date_parser, date_format, thousands, decimal, comment, skipfooter, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    476\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(io, ExcelFile):\n\u001b[0;32m    477\u001b[0m     should_close \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 478\u001b[0m     io \u001b[38;5;241m=\u001b[39m \u001b[43mExcelFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mio\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mengine\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    479\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m engine \u001b[38;5;129;01mand\u001b[39;00m engine \u001b[38;5;241m!=\u001b[39m io\u001b[38;5;241m.\u001b[39mengine:\n\u001b[0;32m    480\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    481\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEngine should not be specified when passing \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    482\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124man ExcelFile - ExcelFile already has the engine set\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    483\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\mccow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\excel\\_base.py:1513\u001b[0m, in \u001b[0;36mExcelFile.__init__\u001b[1;34m(self, path_or_buffer, engine, storage_options)\u001b[0m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine \u001b[38;5;241m=\u001b[39m engine\n\u001b[0;32m   1511\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstorage_options \u001b[38;5;241m=\u001b[39m storage_options\n\u001b[1;32m-> 1513\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engines\u001b[49m\u001b[43m[\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m]\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_io\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mccow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\excel\\_openpyxl.py:548\u001b[0m, in \u001b[0;36mOpenpyxlReader.__init__\u001b[1;34m(self, filepath_or_buffer, storage_options)\u001b[0m\n\u001b[0;32m    533\u001b[0m \u001b[38;5;129m@doc\u001b[39m(storage_options\u001b[38;5;241m=\u001b[39m_shared_docs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    534\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\n\u001b[0;32m    535\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    536\u001b[0m     filepath_or_buffer: FilePath \u001b[38;5;241m|\u001b[39m ReadBuffer[\u001b[38;5;28mbytes\u001b[39m],\n\u001b[0;32m    537\u001b[0m     storage_options: StorageOptions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[0;32m    538\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    539\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    540\u001b[0m \u001b[38;5;124;03m    Reader using openpyxl engine.\u001b[39;00m\n\u001b[0;32m    541\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    546\u001b[0m \u001b[38;5;124;03m    {storage_options}\u001b[39;00m\n\u001b[0;32m    547\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 548\u001b[0m     \u001b[43mimport_optional_dependency\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mopenpyxl\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    549\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(filepath_or_buffer, storage_options\u001b[38;5;241m=\u001b[39mstorage_options)\n",
      "File \u001b[1;32mc:\\Users\\mccow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\compat\\_optional.py:145\u001b[0m, in \u001b[0;36mimport_optional_dependency\u001b[1;34m(name, extra, errors, min_version)\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[0;32m    144\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 145\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg)\n\u001b[0;32m    146\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    148\u001b[0m \u001b[38;5;66;03m# Handle submodules: if we have submodule, grab parent module from sys.modules\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: Missing optional dependency 'openpyxl'.  Use pip or conda to install openpyxl."
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "dataframe = pd.read_excel(url, sheet_name=0, header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>integer</th>\n",
       "      <th>datetime</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>2015-01-01 00:00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>2015-01-01 00:00:01</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   integer             datetime  category\n",
       "0        5  2015-01-01 00:00:00         0\n",
       "1        5  2015-01-01 00:00:01         0"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View the first two rows\n",
    "dataframe.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This solution is similar to our solution for reading CSV files. The main difference is the additional parameter, sheet_name, that specifies which sheet in the Excel file we wish to load. sheet_name can accept both strings, containing the name of the sheet, and integers, pointing to sheet positions (zero-indexed). If we need to load multiple sheets, we include them as a list. For example, sheet_name=[0,1,2, \"Monthly Sales\"] will return a dictionary of pandas DataFrames containing the first, second, and third sheets, and the sheet named Monthly Sales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading a JSON File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pandas library provides read_json to convert a JSON file into a pandas object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load library\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create URL\n",
    "url = 'https://raw.githubusercontent.com/chrisalbon/sim_data/master/data.json'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "dataframe = pd.read_json(url, orient='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>integer</th>\n",
       "      <th>datetime</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>2015-01-01 00:00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>2015-01-01 00:00:01</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   integer            datetime  category\n",
       "0        5 2015-01-01 00:00:00         0\n",
       "1        5 2015-01-01 00:00:01         0"
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View the first two rows\n",
    "dataframe.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing JSON files into pandas is similar to the last few recipes we have seen. The key difference is the orient parameter, which indicates to pandas how the JSON file is structured. However, it might take some experimenting to figure out which argument (split, records, index, columns, or values) is the right one. Another helpful tool pandas offers is json_normalize, which can help convert semistructured JSON data into a pandas DataFrame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading a Parquet File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The pandas read_parquet function allows us to read in Parquet files:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load library\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create URL\n",
    "url = 'https://machine-learning-python-cookbook.s3.amazonaws.com/data.parquet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Unable to find a usable engine; tried using: 'pyarrow', 'fastparquet'.\nA suitable version of pyarrow or fastparquet is required for parquet support.\nTrying to import the above resulted in these errors:\n - Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.\n - Missing optional dependency 'fastparquet'. fastparquet is required for parquet support. Use pip or conda to install fastparquet.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[149], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Load data\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m dataframe \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_parquet\u001b[49m\u001b[43m(\u001b[49m\u001b[43murl\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mccow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parquet.py:493\u001b[0m, in \u001b[0;36mread_parquet\u001b[1;34m(path, engine, columns, storage_options, use_nullable_dtypes, dtype_backend, **kwargs)\u001b[0m\n\u001b[0;32m    428\u001b[0m \u001b[38;5;129m@doc\u001b[39m(storage_options\u001b[38;5;241m=\u001b[39m_shared_docs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m    429\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_parquet\u001b[39m(\n\u001b[0;32m    430\u001b[0m     path: FilePath \u001b[38;5;241m|\u001b[39m ReadBuffer[\u001b[38;5;28mbytes\u001b[39m],\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    436\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m    437\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[0;32m    438\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    439\u001b[0m \u001b[38;5;124;03m    Load a parquet object from the file path, returning a DataFrame.\u001b[39;00m\n\u001b[0;32m    440\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    491\u001b[0m \u001b[38;5;124;03m    DataFrame\u001b[39;00m\n\u001b[0;32m    492\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 493\u001b[0m     impl \u001b[38;5;241m=\u001b[39m \u001b[43mget_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    495\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m use_nullable_dtypes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mno_default:\n\u001b[0;32m    496\u001b[0m         msg \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    497\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe argument \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124muse_nullable_dtypes\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is deprecated and will be removed \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    498\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min a future version.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    499\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\mccow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parquet.py:60\u001b[0m, in \u001b[0;36mget_engine\u001b[1;34m(engine)\u001b[0m\n\u001b[0;32m     57\u001b[0m         \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m     58\u001b[0m             error_msgs \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m - \u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(err)\n\u001b[1;32m---> 60\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(\n\u001b[0;32m     61\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to find a usable engine; \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     62\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtried using: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfastparquet\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     63\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mA suitable version of \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     64\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow or fastparquet is required for parquet \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     65\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msupport.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     66\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTrying to import the above resulted in these errors:\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     67\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00merror_msgs\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     68\u001b[0m     )\n\u001b[0;32m     70\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m engine \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpyarrow\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     71\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m PyArrowImpl()\n",
      "\u001b[1;31mImportError\u001b[0m: Unable to find a usable engine; tried using: 'pyarrow', 'fastparquet'.\nA suitable version of pyarrow or fastparquet is required for parquet support.\nTrying to import the above resulted in these errors:\n - Missing optional dependency 'pyarrow'. pyarrow is required for parquet support. Use pip or conda to install pyarrow.\n - Missing optional dependency 'fastparquet'. fastparquet is required for parquet support. Use pip or conda to install fastparquet."
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "dataframe = pd.read_parquet(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>integer</th>\n",
       "      <th>datetime</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>2015-01-01 00:00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>2015-01-01 00:00:01</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   integer            datetime  category\n",
       "0        5 2015-01-01 00:00:00         0\n",
       "1        5 2015-01-01 00:00:01         0"
      ]
     },
     "execution_count": 150,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View the first two rows\n",
    "dataframe.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Parquet is a popular data storage format in the large data space. It is often used with big data tools such as Hadoop and Spark. While PySpark is outside the focus of this book, it’s highly likely companies operating on a large scale will use an efficient data storage format such as Parquet, and it’s valuable to know how to read it into a dataframe and manipulate it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading an Avro File"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The use the pandavro library’s read_avro method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pandavro\n",
      "  Downloading pandavro-1.8.0-py3-none-any.whl (8.8 kB)\n",
      "Collecting fastavro<2.0.0,>=1.5.1 (from pandavro)\n",
      "  Downloading fastavro-1.9.4-cp311-cp311-win_amd64.whl (499 kB)\n",
      "                                              0.0/499.4 kB ? eta -:--:--\n",
      "     -------------------                    256.0/499.4 kB 5.2 MB/s eta 0:00:01\n",
      "     -------------------------------------- 499.4/499.4 kB 7.9 MB/s eta 0:00:00\n",
      "Requirement already satisfied: pandas>=1.1 in c:\\users\\mccow\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandavro) (2.0.2)\n",
      "Requirement already satisfied: numpy>=1.15.4 in c:\\users\\mccow\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandavro) (1.24.3)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\mccow\\appdata\\roaming\\python\\python311\\site-packages (from pandas>=1.1->pandavro) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\mccow\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas>=1.1->pandavro) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\mccow\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas>=1.1->pandavro) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\mccow\\appdata\\roaming\\python\\python311\\site-packages (from python-dateutil>=2.8.2->pandas>=1.1->pandavro) (1.16.0)\n",
      "Installing collected packages: fastavro, pandavro\n",
      "Successfully installed fastavro-1.9.4 pandavro-1.8.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# !pip install pandavro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load library\n",
    "import requests\n",
    "import pandavro as pdx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create URL\n",
    "url = 'https://machine-learning-python-cookbook.s3.amazonaws.com/data.avro'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2763"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download file\n",
    "r = requests.get(url)\n",
    "open('data.avro', 'wb').write(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "dataframe = pdx.read_avro('data.avro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>integer</th>\n",
       "      <th>datetime</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>2015-01-01 00:00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>2015-01-01 00:00:01</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   integer             datetime  category\n",
       "0        5  2015-01-01 00:00:00         0\n",
       "1        5  2015-01-01 00:00:01         0"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View the first two rows\n",
    "dataframe.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apache Avro is an open source, binary data format that relies on schemas for the data structure. At the time of writing, it is not as common as Parquet. However, large binary data formats such as Avro, thrift, and Protocol Buffers are growing in popularity due to their efficient nature. If you work with large data systems, you’re likely to run into one of these formats in the near future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying a SQLite Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pandas’ read_sql_query allows us to make an SQL query to a database and load it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting sqlalchemy\n",
      "  Downloading SQLAlchemy-2.0.29-cp311-cp311-win_amd64.whl (2.1 MB)\n",
      "                                              0.0/2.1 MB ? eta -:--:--\n",
      "     -----                                    0.3/2.1 MB 5.9 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.0/2.1 MB 21.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 2.1/2.1 MB 18.8 MB/s eta 0:00:00\n",
      "Collecting typing-extensions>=4.6.0 (from sqlalchemy)\n",
      "  Downloading typing_extensions-4.11.0-py3-none-any.whl (34 kB)\n",
      "Collecting greenlet!=0.4.17 (from sqlalchemy)\n",
      "  Downloading greenlet-3.0.3-cp311-cp311-win_amd64.whl (292 kB)\n",
      "                                              0.0/292.8 kB ? eta -:--:--\n",
      "     ---------------------------------------- 292.8/292.8 kB ? eta 0:00:00\n",
      "Installing collected packages: typing-extensions, greenlet, sqlalchemy\n",
      "Successfully installed greenlet-3.0.3 sqlalchemy-2.0.29 typing-extensions-4.11.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# !pip install sqlalchemy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load libraries\n",
    "import pandas as pd\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a connection to the database\n",
    "database_connection = create_engine('example')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "ename": "OperationalError",
     "evalue": "(sqlite3.OperationalError) no such table: data\n[SQL: SELECT * FROM data]\n(Background on this error at: https://sqlalche.me/e/20/e3q8)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\mccow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:1971\u001b[0m, in \u001b[0;36mConnection._exec_single_context\u001b[1;34m(self, dialect, context, statement, parameters)\u001b[0m\n\u001b[0;32m   1970\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m evt_handled:\n\u001b[1;32m-> 1971\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdialect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_execute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1972\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcursor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstr_statement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meffective_parameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\n\u001b[0;32m   1973\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1975\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_events \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine\u001b[38;5;241m.\u001b[39m_has_events:\n",
      "File \u001b[1;32mc:\\Users\\mccow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sqlalchemy\\engine\\default.py:919\u001b[0m, in \u001b[0;36mDefaultDialect.do_execute\u001b[1;34m(self, cursor, statement, parameters, context)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdo_execute\u001b[39m(\u001b[38;5;28mself\u001b[39m, cursor, statement, parameters, context\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m--> 919\u001b[0m     \u001b[43mcursor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstatement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mOperationalError\u001b[0m: no such table: data",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[162], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Load data\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m dataframe \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_sql_query\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mSELECT * FROM data\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatabase_connection\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mccow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\sql.py:469\u001b[0m, in \u001b[0;36mread_sql_query\u001b[1;34m(sql, con, index_col, coerce_float, params, parse_dates, chunksize, dtype, dtype_backend)\u001b[0m\n\u001b[0;32m    466\u001b[0m     dtype_backend \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m\"\u001b[39m  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[0;32m    468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m pandasSQL_builder(con) \u001b[38;5;28;01mas\u001b[39;00m pandas_sql:\n\u001b[1;32m--> 469\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpandas_sql\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_query\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    470\u001b[0m \u001b[43m        \u001b[49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    471\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex_col\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex_col\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    472\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    473\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcoerce_float\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcoerce_float\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    474\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparse_dates\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparse_dates\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    475\u001b[0m \u001b[43m        \u001b[49m\u001b[43mchunksize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mchunksize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    476\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    477\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdtype_backend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype_backend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    478\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mccow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\sql.py:1738\u001b[0m, in \u001b[0;36mSQLDatabase.read_query\u001b[1;34m(self, sql, index_col, coerce_float, parse_dates, params, chunksize, dtype, dtype_backend)\u001b[0m\n\u001b[0;32m   1681\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mread_query\u001b[39m(\n\u001b[0;32m   1682\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1683\u001b[0m     sql: \u001b[38;5;28mstr\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1690\u001b[0m     dtype_backend: DtypeBackend \u001b[38;5;241m|\u001b[39m Literal[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnumpy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   1691\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Iterator[DataFrame]:\n\u001b[0;32m   1692\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1693\u001b[0m \u001b[38;5;124;03m    Read SQL query into a DataFrame.\u001b[39;00m\n\u001b[0;32m   1694\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1736\u001b[0m \n\u001b[0;32m   1737\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1738\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1739\u001b[0m     columns \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mkeys()\n\u001b[0;32m   1741\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\mccow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\sql.py:1562\u001b[0m, in \u001b[0;36mSQLDatabase.execute\u001b[1;34m(self, sql, params)\u001b[0m\n\u001b[0;32m   1560\u001b[0m args \u001b[38;5;241m=\u001b[39m [] \u001b[38;5;28;01mif\u001b[39;00m params \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m [params]\n\u001b[0;32m   1561\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(sql, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcon\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexec_driver_sql\u001b[49m\u001b[43m(\u001b[49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1563\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcon\u001b[38;5;241m.\u001b[39mexecute(sql, \u001b[38;5;241m*\u001b[39margs)\n",
      "File \u001b[1;32mc:\\Users\\mccow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:1783\u001b[0m, in \u001b[0;36mConnection.exec_driver_sql\u001b[1;34m(self, statement, parameters, execution_options)\u001b[0m\n\u001b[0;32m   1778\u001b[0m execution_options \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_execution_options\u001b[38;5;241m.\u001b[39mmerge_with(\n\u001b[0;32m   1779\u001b[0m     execution_options\n\u001b[0;32m   1780\u001b[0m )\n\u001b[0;32m   1782\u001b[0m dialect \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdialect\n\u001b[1;32m-> 1783\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_context\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1784\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdialect\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1785\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdialect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecution_ctx_cls\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_init_statement\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1786\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstatement\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1787\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m   1788\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexecution_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1789\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstatement\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdistilled_parameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1791\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1793\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m ret\n",
      "File \u001b[1;32mc:\\Users\\mccow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:1850\u001b[0m, in \u001b[0;36mConnection._execute_context\u001b[1;34m(self, dialect, constructor, statement, parameters, execution_options, *args, **kw)\u001b[0m\n\u001b[0;32m   1848\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exec_insertmany_context(dialect, context)\n\u001b[0;32m   1849\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1850\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_exec_single_context\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1851\u001b[0m \u001b[43m        \u001b[49m\u001b[43mdialect\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstatement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\n\u001b[0;32m   1852\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mccow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:1990\u001b[0m, in \u001b[0;36mConnection._exec_single_context\u001b[1;34m(self, dialect, context, statement, parameters)\u001b[0m\n\u001b[0;32m   1987\u001b[0m     result \u001b[38;5;241m=\u001b[39m context\u001b[38;5;241m.\u001b[39m_setup_result_proxy()\n\u001b[0;32m   1989\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 1990\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle_dbapi_exception\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1991\u001b[0m \u001b[43m        \u001b[49m\u001b[43me\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstr_statement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meffective_parameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcursor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\n\u001b[0;32m   1992\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1994\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[1;32mc:\\Users\\mccow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:2357\u001b[0m, in \u001b[0;36mConnection._handle_dbapi_exception\u001b[1;34m(self, e, statement, parameters, cursor, context, is_sub_exec)\u001b[0m\n\u001b[0;32m   2355\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m should_wrap:\n\u001b[0;32m   2356\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m sqlalchemy_exception \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 2357\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m sqlalchemy_exception\u001b[38;5;241m.\u001b[39mwith_traceback(exc_info[\u001b[38;5;241m2\u001b[39m]) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m   2358\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2359\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m exc_info[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mccow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sqlalchemy\\engine\\base.py:1971\u001b[0m, in \u001b[0;36mConnection._exec_single_context\u001b[1;34m(self, dialect, context, statement, parameters)\u001b[0m\n\u001b[0;32m   1969\u001b[0m                 \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m   1970\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m evt_handled:\n\u001b[1;32m-> 1971\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdialect\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdo_execute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1972\u001b[0m \u001b[43m            \u001b[49m\u001b[43mcursor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstr_statement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meffective_parameters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcontext\u001b[49m\n\u001b[0;32m   1973\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1975\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_has_events \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine\u001b[38;5;241m.\u001b[39m_has_events:\n\u001b[0;32m   1976\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch\u001b[38;5;241m.\u001b[39mafter_cursor_execute(\n\u001b[0;32m   1977\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   1978\u001b[0m         cursor,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1982\u001b[0m         context\u001b[38;5;241m.\u001b[39mexecutemany,\n\u001b[0;32m   1983\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\mccow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\sqlalchemy\\engine\\default.py:919\u001b[0m, in \u001b[0;36mDefaultDialect.do_execute\u001b[1;34m(self, cursor, statement, parameters, context)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdo_execute\u001b[39m(\u001b[38;5;28mself\u001b[39m, cursor, statement, parameters, context\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m--> 919\u001b[0m     \u001b[43mcursor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstatement\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mOperationalError\u001b[0m: (sqlite3.OperationalError) no such table: data\n[SQL: SELECT * FROM data]\n(Background on this error at: https://sqlalche.me/e/20/e3q8)"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "dataframe = pd.read_sql_query('SELECT * FROM data', database_connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View first two rows\n",
    "dataframe.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SQL is the lingua franca for pulling data from databases. In this recipe we first use create_engine to define a connection to an SQL database engine called SQLite. Next we use pandas’ read_sql_query to query that database using SQL and put the results in a DataFrame.\n",
    "\n",
    "SQL is a language in its own right and, while beyond the scope of this book, it is certainly worth knowing for anyone wanting to learn about machine learning. Our SQL query, SELECT * FROM data, asks the database to give us all columns (*) from the table called data.\n",
    "\n",
    "Note that this is one of a few recipes in this book that will not run without extra code. Specifically, create_engine('sqlite:///sample.db') assumes that an SQLite database already exists."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Querying a Remote SQL Database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a connection with pymysql and read it into a dataframe with pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pymysql\n",
      "  Downloading PyMySQL-1.1.0-py3-none-any.whl (44 kB)\n",
      "                                              0.0/44.8 kB ? eta -:--:--\n",
      "     ---------------------------------------- 44.8/44.8 kB 2.2 MB/s eta 0:00:00\n",
      "Installing collected packages: pymysql\n",
      "Successfully installed pymysql-1.1.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.1.2 -> 24.0\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# !pip install pymysql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pymysql\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "ename": "OperationalError",
     "evalue": "(2003, \"Can't connect to MySQL server on 'localhost' ([WinError 10061] No connection could be made because the target machine actively refused it)\")",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mConnectionRefusedError\u001b[0m                    Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\mccow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pymysql\\connections.py:644\u001b[0m, in \u001b[0;36mConnection.connect\u001b[1;34m(self, sock)\u001b[0m\n\u001b[0;32m    643\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 644\u001b[0m     sock \u001b[38;5;241m=\u001b[39m \u001b[43msocket\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcreate_connection\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    645\u001b[0m \u001b[43m        \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhost\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mport\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect_timeout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[0;32m    646\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    647\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\mccow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\socket.py:851\u001b[0m, in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, all_errors)\u001b[0m\n\u001b[0;32m    850\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m all_errors:\n\u001b[1;32m--> 851\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exceptions[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m    852\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m ExceptionGroup(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcreate_connection failed\u001b[39m\u001b[38;5;124m\"\u001b[39m, exceptions)\n",
      "File \u001b[1;32mc:\\Users\\mccow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\socket.py:836\u001b[0m, in \u001b[0;36mcreate_connection\u001b[1;34m(address, timeout, source_address, all_errors)\u001b[0m\n\u001b[0;32m    835\u001b[0m     sock\u001b[38;5;241m.\u001b[39mbind(source_address)\n\u001b[1;32m--> 836\u001b[0m \u001b[43msock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43msa\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    837\u001b[0m \u001b[38;5;66;03m# Break explicitly a reference cycle\u001b[39;00m\n",
      "\u001b[1;31mConnectionRefusedError\u001b[0m: [WinError 10061] No connection could be made because the target machine actively refused it",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mOperationalError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[166], line 4\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Create a DB connection\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Use the following example to start a DB instance\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m# https://github.com/kylegallatin/mysql-db-example\u001b[39;00m\n\u001b[1;32m----> 4\u001b[0m conn \u001b[38;5;241m=\u001b[39m \u001b[43mpymysql\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhost\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mlocalhost\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m    \u001b[49m\u001b[43muser\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mroot\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      7\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdb\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mccow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pymysql\\connections.py:358\u001b[0m, in \u001b[0;36mConnection.__init__\u001b[1;34m(self, user, password, host, database, unix_socket, port, charset, collation, sql_mode, read_default_file, conv, use_unicode, client_flag, cursorclass, init_command, connect_timeout, read_default_group, autocommit, local_infile, max_allowed_packet, defer_connect, auth_plugin_map, read_timeout, write_timeout, bind_address, binary_prefix, program_name, server_public_key, ssl, ssl_ca, ssl_cert, ssl_disabled, ssl_key, ssl_verify_cert, ssl_verify_identity, compress, named_pipe, passwd, db)\u001b[0m\n\u001b[0;32m    356\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sock \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    357\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 358\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconnect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mccow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pymysql\\connections.py:711\u001b[0m, in \u001b[0;36mConnection.connect\u001b[1;34m(self, sock)\u001b[0m\n\u001b[0;32m    709\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m DEBUG:\n\u001b[0;32m    710\u001b[0m         \u001b[38;5;28mprint\u001b[39m(exc\u001b[38;5;241m.\u001b[39mtraceback)\n\u001b[1;32m--> 711\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[0;32m    713\u001b[0m \u001b[38;5;66;03m# If e is neither DatabaseError or IOError, It's a bug.\u001b[39;00m\n\u001b[0;32m    714\u001b[0m \u001b[38;5;66;03m# But raising AssertionError hides original error.\u001b[39;00m\n\u001b[0;32m    715\u001b[0m \u001b[38;5;66;03m# So just reraise it.\u001b[39;00m\n\u001b[0;32m    716\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mOperationalError\u001b[0m: (2003, \"Can't connect to MySQL server on 'localhost' ([WinError 10061] No connection could be made because the target machine actively refused it)\")"
     ]
    }
   ],
   "source": [
    "# Create a DB connection\n",
    "# Use the following example to start a DB instance\n",
    "# https://github.com/kylegallatin/mysql-db-example\n",
    "conn = pymysql.connect(\n",
    "    host='localhost',\n",
    "    user='root',\n",
    "    password = \"\",\n",
    "    db='db',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the SQL query into a dataframe\n",
    "dataframe = pd.read_sql(\"select * from data\", conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>integer</th>\n",
       "      <th>datetime</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>2015-01-01 00:00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>2015-01-01 00:00:01</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   integer             datetime  category\n",
       "0        5  2015-01-01 00:00:00         0\n",
       "1        5  2015-01-01 00:00:01         0"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View the first two rows\n",
    "dataframe.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of all of the recipes presented in this chapter, this is probably the one we will use most in the real world. While connecting and reading from an example sqlite database is useful, it’s likely not representative of tables you’ll need to connect to in an enterprise environment. Most SQL instances that you’ll connect to will require you to connect to the host and port of a remote machine, specifying a username and password for authentication. This example requires you to start a running SQL instance locally that mimics a remote server on localhost so that you can get a sense of the workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data from a Google Sheet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use pandas read_CSV and pass a URL that exports the Google Sheet as a CSV:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Sheet URL that downloads the sheet as a CSV\n",
    "url = \"https://docs.google.com/spreadsheets/d/\"\\\n",
    "      \"1ehC-9otcAuitqnmWksqt1mOrTRCL38dv0K9UjhwzTOA/export?format=csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the CSV into a dataframe\n",
    "dataframe = pd.read_csv(url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>integer</th>\n",
       "      <th>datetime</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5</td>\n",
       "      <td>2015-01-01 0:00:00</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>2015-01-01 0:00:01</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   integer            datetime  category\n",
       "0        5  2015-01-01 0:00:00         0\n",
       "1        5  2015-01-01 0:00:01         0"
      ]
     },
     "execution_count": 171,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# View the first two rows\n",
    "dataframe.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While Google Sheets can easily be downloaded, it’s sometimes helpful to be able to read them directly into Python without any intermediate steps. The /export?format=csv query parameter at the end of the URL above creates an endpoint from which we can either download the file or read it into pandas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Data from an S3 Bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add storage options to pandas giving it access to the S3 object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S3 path to CSV\n",
    "s3_uri = \"s3://machine-learning-python-cookbook/data.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set AWS credentials (replace with your own)\n",
    "ACCESS_KEY_ID = \"xxxxxxxxxxxxx\"\n",
    "SECRET_ACCESS_KEY = \"xxxxxxxxxxxxxxxx\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Missing optional dependency 'fsspec'.  Use pip or conda to install fsspec.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\mccow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\compat\\_optional.py:142\u001b[0m, in \u001b[0;36mimport_optional_dependency\u001b[1;34m(name, extra, errors, min_version)\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 142\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\mccow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\importlib\\__init__.py:126\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m    125\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m--> 126\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1206\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1178\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1142\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'fsspec'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[175], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Read the CSV into a dataframe\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m dataframe \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43ms3_uri\u001b[49m\u001b[43m,\u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mkey\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mACCESS_KEY_ID\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43msecret\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mSECRET_ACCESS_KEY\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m  \u001b[49m\u001b[43m}\u001b[49m\n\u001b[0;32m      6\u001b[0m \u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mccow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:912\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m    899\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    900\u001b[0m     dialect,\n\u001b[0;32m    901\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    908\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m    909\u001b[0m )\n\u001b[0;32m    910\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 912\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mccow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:577\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    574\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    576\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 577\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    579\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    580\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\mccow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1407\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1404\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1406\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1407\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\mccow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1661\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1659\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1660\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1661\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1662\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1663\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1664\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1665\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1666\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1667\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1668\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1669\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1670\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1671\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1672\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32mc:\\Users\\mccow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\common.py:716\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    713\u001b[0m     codecs\u001b[38;5;241m.\u001b[39mlookup_error(errors)\n\u001b[0;32m    715\u001b[0m \u001b[38;5;66;03m# open URLs\u001b[39;00m\n\u001b[1;32m--> 716\u001b[0m ioargs \u001b[38;5;241m=\u001b[39m \u001b[43m_get_filepath_or_buffer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    717\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpath_or_buf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    718\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    719\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompression\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    720\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    721\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    722\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    724\u001b[0m handle \u001b[38;5;241m=\u001b[39m ioargs\u001b[38;5;241m.\u001b[39mfilepath_or_buffer\n\u001b[0;32m    725\u001b[0m handles: \u001b[38;5;28mlist\u001b[39m[BaseBuffer]\n",
      "File \u001b[1;32mc:\\Users\\mccow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\io\\common.py:393\u001b[0m, in \u001b[0;36m_get_filepath_or_buffer\u001b[1;34m(filepath_or_buffer, encoding, compression, mode, storage_options)\u001b[0m\n\u001b[0;32m    391\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m filepath_or_buffer\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ms3n://\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    392\u001b[0m     filepath_or_buffer \u001b[38;5;241m=\u001b[39m filepath_or_buffer\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ms3n://\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124ms3://\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 393\u001b[0m fsspec \u001b[38;5;241m=\u001b[39m \u001b[43mimport_optional_dependency\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mfsspec\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    395\u001b[0m \u001b[38;5;66;03m# If botocore is installed we fallback to reading with anon=True\u001b[39;00m\n\u001b[0;32m    396\u001b[0m \u001b[38;5;66;03m# to allow reads from public buckets\u001b[39;00m\n\u001b[0;32m    397\u001b[0m err_types_to_retry_with_anon: \u001b[38;5;28mlist\u001b[39m[Any] \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\mccow\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pandas\\compat\\_optional.py:145\u001b[0m, in \u001b[0;36mimport_optional_dependency\u001b[1;34m(name, extra, errors, min_version)\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n\u001b[0;32m    144\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m--> 145\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(msg)\n\u001b[0;32m    146\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    148\u001b[0m \u001b[38;5;66;03m# Handle submodules: if we have submodule, grab parent module from sys.modules\u001b[39;00m\n",
      "\u001b[1;31mImportError\u001b[0m: Missing optional dependency 'fsspec'.  Use pip or conda to install fsspec."
     ]
    }
   ],
   "source": [
    "# Read the CSV into a dataframe\n",
    "dataframe = pd.read_csv(s3_uri,storage_options={\n",
    "        \"key\": ACCESS_KEY_ID,\n",
    "        \"secret\": SECRET_ACCESS_KEY,\n",
    "  }\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# View first two rows\n",
    "dataframe.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many enterprises now keep data in cloud provider blob stores such as Amazon S3 or Google Cloud Storage (GCS). It’s common for machine learning practitioners to connect to these sources to retrieve data. Although the S3 URI (s3://machine-learning-python-cookbook/data.csv) is public, it still requires you to provide your own AWS access credentials to access it. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Unstructured Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the base Python open function to load the information:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# URL to download the txt file from\n",
    "txt_url = \"https://machine-learning-python-cookbook.s3.amazonaws.com/text.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the txt file\n",
    "r = requests.get(txt_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write it to text.txt locally\n",
    "with open('text.txt', 'wb') as f:\n",
    "    f.write(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the file\n",
    "with open('text.txt', 'r') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello there!\n"
     ]
    }
   ],
   "source": [
    "# Print the content\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While structured data can easily be read in from CSV, JSON, or various databases, unstructured data can be more challenging and may require custom processing down the line. Sometimes it’s helpful to open and read in files using Python’s basic open function. This allows us to open files and then read the content of that file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Wrangling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
